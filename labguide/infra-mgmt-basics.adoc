## Infrastructure Management, Metrics and Logging
In this lab you will explore various aspects of managing cluster infrastructure.
This includes extending the OpenShift cluster and installation of the
Logging and Metrics components, all automated by the installer. It also includes
some maintenance of nodes, as well as manipulating the multitenant network.

[NOTE]
====
It is recommended that you `sudo -i` to `root` before performing these exercises.
====

### Quick Background on Nodes, Groups and Selectors

Log in as cluster administrator:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

Verify your cluster currently consists of 5 nodes. Remember that the Master
is also a node, and that we have a host dedicated to running OpenShift's
infrastructure.

[source,bash,role="copypaste"]
----
oc get nodes
----

You should see:

----
NAME                                          STATUS    ROLES     AGE       VERSION
{{ INFRA_INTERNAL_FQDN }}    Ready     infra     9m        v1.11.0+d4cacc0
{{ MASTER_INTERNAL_FQDN }}   Ready     master    15m       v1.11.0+d4cacc0
{{ NODE1_INTERNAL_FQDN }}   Ready     compute   9m        v1.11.0+d4cacc0
{{ NODE2_INTERNAL_FQDN }}   Ready     compute   9m        v1.11.0+d4cacc0
{{ NODE3_INTERNAL_FQDN }}   Ready     compute   9m        v1.11.0+d4cacc0
----

In the App Management lab we talked a little bit about `Selectors` as they
are used with `Services` to find `Pods`. But there is also a concept of a
`NodeSelector`. It is used to tell a `Pod` which `Node` it should run on.
During the installation process, when you don't specify anything, OpenShift
will configure a default `NodeSelector`. You can find it with the following
command:

[source,bash,role="copypaste"]
----
grep defaultNodeSelector /etc/origin/master/master-config.yaml
----

You'll see:

[source,yaml]
----
defaultNodeSelector: node-role.kubernetes.io/compute=true
----

This looks like the `key=value` syntax we are used to seeing when we have
talked previously about `Labels`. Execute the following:

[source,bash,role="copypaste"]
----
oc get node --show-labels
----

You'll see something like:

----
NAME                                          STATUS    ROLES     AGE       VERSION           LABELS
{{ INFRA_INTERNAL_FQDN }}    Ready     infra     14m       v1.11.0+d4cacc0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infra.internal.aws.testdrive.openshift.com,node-role.kubernetes.io/infra=true
{{ MASTER_INTERNAL_FQDN }}   Ready     master    20m       v1.11.0+d4cacc0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.internal.aws.testdrive.openshift.com,node-role.kubernetes.io/master=true
{{ NODE1_INTERNAL_FQDN }}   Ready     compute   14m       v1.11.0+d4cacc0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node01.internal.aws.testdrive.openshift.com,node-role.kubernetes.io/compute=true
{{ NODE2_INTERNAL_FQDN }}   Ready     compute   14m       v1.11.0+d4cacc0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node02.internal.aws.testdrive.openshift.com,node-role.kubernetes.io/compute=true
{{ NODE3_INTERNAL_FQDN }}   Ready     compute   14m       v1.11.0+d4cacc0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node03.internal.aws.testdrive.openshift.com,node-role.kubernetes.io/compute=true
----

[NOTE]
====
The above has line wraps in it, especially at larger font sizes. To get slightly reduced output you can also use the following command:

[source,bash,role="copypaste"]
----
oc get node --show-labels | awk '{ print $1, $6 }' | column -t
----
====

Every node has a `Label` for its role, as well as some extra labels that
specify other details about the host. Where do these `Labels` come from?
Execute the following:

[source,bash,role="copypaste"]
----
grep openshift_node_group_name /etc/ansible/hosts
----

You will see that each node has a node group name specified. The OpenShift
installer has several node group definitions built-in for things like
`compute` nodes (the generic hosts where all your apps will run), `masters`,
`infra` nodes, and etc. If you want to define additional types, specify
additional kubelet arguments, add additional labels to hosts, or similar
things, you will need to define custom node groups. This is quite trivial,
and you can find out how to do it in the
link:https://docs.openshift.com/container-platform/latest/install/configuring_inventory_file.html#configuring-inventory-defining-node-group-and-host-mappings[documentation for node groups].

Thinking back to the previous paragraphs, since the default `NodeSelector` is
`node-role.kubernetes.io/compute=true`, we can expect that pods without a
`NodeSelector` specified will only end up on these `compute` hosts.

### Extending the Cluster
Extending the cluster is easy. Simply add a new set of hosts to an Ansible group
called `new_nodes` in the `openshift-ansible` installer's inventory. Then, run
the `scaleup` playbook.

#### Configure the Installer
Your environment already has 3 additional nodes provisioned, but you have not used
them so far. They are already configured in the inventory file, but commented out with a `#scaleup_` prefix.

To see the lines run:

[source,bash,role="copypaste"]
----
grep '#scaleup_' /etc/ansible/hosts
----

Remove the `#scaleup_` comment prefix by running the below `sed` command:

[source,bash,role="copypaste"]
----
sudo sed -i 's/#scaleup_//g' /etc/ansible/hosts
----

When finished, your inventory file should look like the following:

[source,ini]
./etc/ansible/hosts
----
[OSEv3:children]
masters
nodes
etcd
glusterfs
new_nodes

...

[new_nodes]
{{ NODE4_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE4_INTERNAL_FQDN }} openshift_public_hostname={{ NODE4_EXTERNAL_FQDN }}
{{ NODE5_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE5_INTERNAL_FQDN }} openshift_public_hostname={{ NODE5_EXTERNAL_FQDN }}
{{ NODE6_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE6_INTERNAL_FQDN }} openshift_public_hostname={{ NODE6_EXTERNAL_FQDN }}

...
----

Now that these hosts are properly defined (uncommented), you can use Ansible to
verify that they are, in fact, online:

[source,bash,role="copypaste"]
----
ansible new_nodes -m ping
----

You will see:

----
{{ NODE5_INTERNAL_FQDN }} | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
{{ NODE4_INTERNAL_FQDN }} | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
{{ NODE6_INTERNAL_FQDN }} | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
----

Much like when you installed OpenShift originally, these new hosts have all of
the
link:https://docs.openshift.com/container-platform/3.11/install_config/install/prerequisites.html[prerequisites]
already taken care of.

#### Run the Playbook to Extend the Cluster
To extend your cluster run the following playbook:

[source,bash,role="copypaste"]
----
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-node/scaleup.yml
----

The playbook takes 1-2 minutes to complete. When done, you can verify that there are now 6 `compute` nodes:

[source,bash,role="copypaste"]
----
oc get nodes -l node-role.kubernetes.io/compute=true
----

You will see:

----
NAME                                          STATUS    ROLES     AGE       VERSION
{{ NODE1_INTERNAL_FQDN }}   Ready     compute   1h        v1.11.0+d4cacc0
{{ NODE2_INTERNAL_FQDN }}   Ready     compute   1h        v1.11.0+d4cacc0
{{ NODE3_INTERNAL_FQDN }}   Ready     compute   1h        v1.11.0+d4cacc0
{{ NODE4_INTERNAL_FQDN }}   Ready     compute   18m       v1.11.0+d4cacc0
{{ NODE5_INTERNAL_FQDN }}   Ready     compute   18m       v1.11.0+d4cacc0
{{ NODE6_INTERNAL_FQDN }}   Ready     compute   18m       v1.11.0+d4cacc0
----

[TIP]
====
When deploying a highly-available multi-master OpenShift environment, it is
also possible to add new master nodes. There is a similar playbook to run. For
more information on multi-master and HA setups, please refer to the link:https://docs.openshift.com/container-platform/3.11/architecture/infrastructure_components/kubernetes_infrastructure.html#high-availability-masters[documentation^].
====

After the scaleup succeeds you need to remove the `new_nodes` entry from [osev3:children]. You also need to remove the '[new_nodes]' section to add the new nodes to the regular [nodes] section of the inventory file.

Check the two lines that got added to enable the scaleup operation:
----
grep new_nodes /etc/ansible/hosts
----

You will see:

----
new_nodes
[new_nodes]
----

Remove [new_nodes] to add new nodes to the [nodes] section in the inventory file. 

----
sudo sed -i '/^\[new_nodes/d' /etc/ansible/hosts
----

Remove new_nodes from [osev3:children] section of the inventory file.

----
sudo sed -i '/^new_nodes/d' /etc/ansible/hosts
----

Your modified inventory file should now look like this:

----
[OSEv3:children]
masters
nodes
etcd
glusterfs
#ocsinfra_glusterfs_registry

...

[nodes]
master.internal.aws.testdrive.openshift.com openshift_public_hostname=master.604368013992.aws.testdrive.openshift.com openshift_node_group_name='node-config-master'
infra.internal.aws.testdrive.openshift.com openshift_public_hostname=infra.604368013992.aws.testdrive.openshift.com openshift_node_group_name='node-config-infra'
node01.internal.aws.testdrive.openshift.com openshift_public_hostname=node01.604368013992.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node02.internal.aws.testdrive.openshift.com openshift_public_hostname=node02.604368013992.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node03.internal.aws.testdrive.openshift.com openshift_public_hostname=node03.604368013992.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'

node04.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node04.604368013992.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node05.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node05.604368013992.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node06.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node06.604368013992.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'

...
----

### OpenShift Container Storage for OpenShift Infrastructure

OpenShift infrastructure, like the Registry, Logging and Metrics (introduced in the following paragraphs in this module) have a requirement for reliable storage. +
OpenShift Container Storage can be configured directly in the installer, to provide a separate storage pool just for those workloads. This is a good practice in order to separate failure domains.

By definition you need a separate set of hosts for this - which you have just made available in the previous paragraph.

#### Configure the Installer
Several directives for a second, infrastructure-centric OCS cluster are in the `/etc/ansible/hosts` file. They have been prepared but commented out using the `#ocsinfra_` prefix.

To see the lines run:

[source,bash,role="copypaste"]
----
grep '#ocsinfra_' /etc/ansible/hosts
----

Remove the `#ocsinfra_` comment prefix by running the below `sed` command:

[source,bash,role="copypaste"]
----
sudo sed -i 's/#ocsinfra_//g' /etc/ansible/hosts
----

When finished, your inventory file should look like the following:

[source,ini]
./etc/ansible/hosts
----
[OSEv3:children]
masters
nodes
etcd
glusterfs
glusterfs_registry

...

[OSEv3:vars]
...
openshift_storage_glusterfs_registry_namespace=infra-storage <1>
openshift_storage_glusterfs_registry_storageclass=true <2>
openshift_storage_glusterfs_registry_block_deploy=true <3>
openshift_storage_glusterfs_registry_block_storageclass=true <4>
openshift_storage_glusterfs_registry_block_host_vol_create=true <5>
openshift_storage_glusterfs_registry_block_host_vol_size=30 <6>

...

[glusterfs_registry] <7>
{{ NODE4_INTERNAL_FQDN }} glusterfs_ip={{ NODE4_INTERNAL_IP }} glusterfs_zone=1 glusterfs_devices='[ "/dev/xvdd" ]' <8>
{{ NODE5_INTERNAL_FQDN }} glusterfs_ip={{ NODE5_INTERNAL_IP }} glusterfs_zone=2 glusterfs_devices='[ "/dev/xvdd" ]'
{{ NODE6_INTERNAL_FQDN }} glusterfs_ip={{ NODE5_INTERNAL_IP }} glusterfs_zone=3 glusterfs_devices='[ "/dev/xvdd" ]'

...
----
<1> Deploys a resource of the OCS cluster for infrastructure in a separate namespace
<2> Creates a StorageClass for the OCS infra cluster
<3> Enables support for block storage - the supported storage option for Logging and Metrics
<4> Creates a StorageClass for the block storage service in the OCS infra cluster
<5> Automatically create block-hosting volumes (see OCS module for further explanations)
<6> Allocate a total of 30GiB for block storage based volumes from the OCS infra cluster
<7> An additional group of hosts which form the OCS infra cluster
<8> Each line is a node with a device list, consumed by OCS

#### Install the OCS cluster for OpenShift infrastructure

To illustrate what becomes available with this step, first look at the
`StorageClass` definitions in the system as of now:

[source,bash,role="copypaste"]
----
oc get sc
----

There is only a single `StorageClass` defined, the default OCS cluster that
shipped with this installation:

----
NAME                          PROVISIONER                AGE
glusterfs-storage (default)   kubernetes.io/glusterfs    10m
----

Don't worry about the concept of the `StorageClass` - we will explain it in more detail later.

With all required lines uncommented you can start the deployment of the
second OCS cluster, dedicated to OpenShift infrastructure workloads:

[source,bash,role="copypaste"]
----
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-glusterfs/config.yml
----

This playbook takes about 1-2 minutes to execute and will install an entirely
independent OCS cluster, including a separate `heketi` management stack.
Additional `StorageClasses` will be set up to make this storage usable.

Verify by using the `oc get sc` command that two new `StorageClasses` are available:

----
NAME                          PROVISIONER                AGE
glusterfs-registry            kubernetes.io/glusterfs    42s <1>
glusterfs-registry-block      gluster.org/glusterblock   24s <2>
glusterfs-storage (default)   kubernetes.io/glusterfs    15m
----
<1> The `StorageClass` representing shared file storage from the OCS infra cluster
<2> The `StorageClass` representing block storage from the OCS infra cluster

The block storage service (identified by the `gluster.org/glusterblock`
provisioner) provided by this second OCS cluster will be explained in the
following module. For now, it's only important to know that for OCS serving
storage Logging and Metrics, the OCS block storage service is the *only*
supported option. The regular file storage service of OCS (identified by
the `kubernetes.io/glusterfs` provisioner) is *not supported* for Logging and
Metrics.

The target use case for these additional storage services is to provide
robust, persistent storage for the Registry, Logging and Metrics service -
the latter 2 we will set up now.

### OpenShift Metrics
_Metrics_ in OpenShift refers to the continuous collection of performance and
utilization data of pods in the cluster. It allows for centralized monitoring in
the OpenShift UI and automated horizontal scaling of pods based on utilization.

The metrics implementation is based on http://www.hawkular.org/[Hawkular], a
metrics collection system running on OpenShift persisting data in a Cassandra
database.

In your environment metrics is not yet deployed. Configuration is done by
customizing the Ansible inventory file `/etc/ansible/hosts` and deployment is
facilitated by running a specific playbook that is part of the
`openshift-ansible` installer. You could have chosen to install the metrics
solution when the cluster was initially installed.

#### Configure the Installer
The lines to configure OpenShift Metrics are already configured in the
inventory file but commented out with a `#metrics_` prefix.

To see the lines run:

[source,bash,role="copypaste"]
----
grep '#metrics_' /etc/ansible/hosts
----

Remove the `#metrics_` comment prefix by running the below `sed` command:

[source,bash,role="copypaste"]
----
sudo sed -i 's/#metrics_//g' /etc/ansible/hosts
----

The OpenShift installer variable `openshift_metrics_install_metrics=false` tells the
installer to *not* install the metrics solution when it runs. Remove that line by
running the below `sed` command:

[source,bash,role="copypaste"]
----
sudo sed -i '/openshift_metrics_install_metrics=false/d' /etc/ansible/hosts
----

When finished, your inventory file should look like the following:

[source,ini]
./etc/ansible/hosts
----
...
[OSEv3:vars]
...
openshift_metrics_install_metrics=true <1>
openshift_metrics_cassandra_storage_type=dynamic <2>
openshift_metrics_storage_volume_size=10Gi <3>
openshift_metrics_hawkular_hostname=metrics.{{ OCP_ROUTING_SUFFIX }} <4>
openshift_metrics_cassanda_pvc_storage_class_name= {{ CNS_BLOCK_STORAGECLASS }} <5>
...
----
<1> Instruct the installer to actually deploy the Metrics service
<2> Cassandra, part of the Metrics service, will get dynamically provisioned storage
<3> The resulting PersistentVolumeClaim will be of `10Gi` in size
<4> The name of the StorageClass to use for the PersistentVolumeClaim, makes it use block storage from OCS
<5> The Metrics frontend (`hawkular`) will be reachable under this domain.

#### Install Metrics
There is a specific playbook included with the installer that will handle metrics. It can be run like so:

[source,bash,role="copypaste"]
----
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-metrics/config.yml
----

This will deploy the metric collection and visualization stack on OpenShift. All
resources will be stood up in the `openshift-infra` *Project*. As part of the
deployment, persistent storage will automatically be used for storing the metrics
information. It will take roughly 2 minutes to complete.

Once the installation playbook has completed, you can then verify that the
metrics components are running in the `openshift-infra` *Project*:

[source,bash,role="copypaste"]
----
oc login -u system:admin -n openshift-infra
oc get pods -o wide
----

It might take a while but after some time you will see something like:

----
NAME                            READY     STATUS      RESTARTS   AGE       IP            NODE                                          NOMINATED NODE
hawkular-cassandra-1-gmqv8      1/1       Running     0          4m        10.129.0.19   {{ INFRA_INTERNAL_FQDN }}    <none>
hawkular-metrics-schema-llf7v   0/1       Completed   0          4m        10.129.2.4    {{ INFRA_INTERNAL_FQDN }}    <none>
hawkular-metrics-sv5mb          1/1       Running     0          4m        10.129.0.17   {{ INFRA_INTERNAL_FQDN }}    <none>
heapster-z9lgv                  1/1       Running     0          4m        10.129.0.18   {{ INFRA_INTERNAL_FQDN }}    <none>

----

You will also see the storage for Cassandra being automatically provisioned
from the OCS block storage service if you query the PersistentVolumeClaim
objects in this project using `oc get pvc`:

----
NAME                  STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS               AGE
metrics-cassandra-1   Bound     pvc-e289ba7c-6af6-11e8-af61-02cea7838d26   10Gi       RWO            {{ CNS_BLOCK_STORAGECLASS }}   3m
----

[NOTE]
====
In this lab environment it can take up to 2-3 minutes after the metrics playbook
finishes for the metrics stack to finish initialization and for all pods to reach
the _Ready_ state.
====

In the `NODE` column you will notice that the *Pods* for Metrics are all located on the infra node. This is because we have set the `NodeSelector` for all of the metrics components using the following bits of the Ansible `hosts` file:

[source,ini]
----
openshift_metrics_hawkular_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_metrics_cassandra_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_metrics_heapster_nodeselector={"node-role.kubernetes.io/infra": "true"}
----

#### Explore the Metrics UI
If you don't have it open, return to the OpenShift web console:

*link:{{ WEB_CONSOLE_URL }}[]*

You will want to be sure you are logged in as `fancyuser1` with the password
`openshift`, who is a `cluster-reader` and can see interesting *Projects*.

Click on the `default` project.

[IMPORTANT]
====
At this point the OpenShift UI will display an error message, stating
that the metrics URL could not be reached:

image:openshift-metrics-url-error.png[]

This is because OpenShift generated a self-signed certificate for the Hawkular
API. Go ahead and click the metrics URL https://metrics.{{ OCP_ROUTING_SUFFIX }}/
to access Hawkular and accept the untrusted certificate. Then, return to the
OpenShift web console and refresh the page, and the metrics should begin to
display.

When working properly and the resource view is expanded, it looks like this:

.The OpenShift UI will show history metrics for applications
image::openshift-metrics-overview.png[]
====

In the context of a specific *Pod*, the _Metrics_ tab in the UI will show CPU,
memory and network throughput for this particular *Pod* with a configurable
time-range. Also optionally a _donut_ chart next to a resource appears if the
pod was given a consumption limit on this resource (e.g. RAM).

image::openshift-metrics-pods.png[]

If you want to see interesting metrics, explore the *Project* for metrics
itself, `openshift-infra`.

### OpenShift Logging
Equally important to performance metrics is collecting and aggregating logs from
the environments and the application pods it is running. OpenShift ships with an
elastic log aggregation solution: *EFK*. **E**lasticSearch, **F**luentd and
**K**ibana form a configuration where logs from all nodes and applications are
consolidated (Fluentd) in a central place (ElasticSearch) on top of which rich
queries can be made from a single UI (Kibana). Administrators can see and search
through all logs. Application owners and developers can allow access to logs that
belong to their projects. Like metrics the EFK stack runs on top of OpenShift.

#### Configuring the Inventory
The lines to configure OpenShift Logging are already configured in the inventory file but commented out with a `#logging_` prefix.

To see the lines run:

[source,bash,role="copypaste"]
----
grep '#logging_' /etc/ansible/hosts
----

Remove the `#logging_` comment prefix by running the below `sed` command:

[source,bash,role="copypaste"]
----
sudo sed -i 's/#logging_//g' /etc/ansible/hosts
----

The OpenShift installer variable `openshift_logging_install_logging=false` tells the
installer to *not* install the logging solution when it runs. Remove that line by
running the below `sed` command:

[source,bash,role="copypaste"]
----
sudo sed -i '/openshift_logging_install_logging=false/d' /etc/ansible/hosts
----

When finished, your inventory file should look like the following:

[source,ini]
./etc/ansible/hosts
----

...

[OSEv3:vars]
...
openshift_logging_install_logging=true <1>
openshift_logging_es_pvc_dynamic=true <2>
openshift_logging_es_pvc_size=10Gi <3>
openshift_logging_es_pvc_storage_class_name={{ CNS_BLOCK_STORAGECLASS }} <4>
openshift_logging_es_memory_limit=2G <5>
openshift_logging_kibana_hostname=kibana.{{ OCP_ROUTING_SUFFIX }} <6>
openshift_logging_curator_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra": "true"}
...
----

<1> Trigger the installation of the Logging service
<2> `ElasticSearch`, part of the Logging service, will request persistent storage for Logging via a claim toward `StorageClass`
<3> The resulting PersistentVolumeClaim will be of `10Gi` in size
<4> The name of the StorageClass to use for the PersistentVolumeClaim
<5> Limit the required memory for the `ElasticSearch` pods to 2GB (refer to the link:https://docs.openshift.com/container-platform/3.11/install_config/aggregate_logging_sizing.html[official docs] for guidance in production environment)
<6> The FQDN under which the Logging frontend UI (Kibana) will be available

#### Install Logging
With these settings in place execute the `openshift-logging` Ansible playbook
that ships as part of the `openshift-ansible` installer:

[source,bash,role="copypaste"]
----
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-logging/config.yml
----

Once the installation finishes (roughly 4 minutes), log in as the cluster administrator, using the
`openshift-logging` *Project*:

[source,bash,role="copypaste"]
----
oc login -u system:admin -n openshift-logging
----

Verify the logging stack components are up and running:

[source,bash,role="copypaste"]
----
oc get pods -o wide
----

You will see something like:

----
NAME                                      READY     STATUS    RESTARTS   AGE       IP            NODE                                          NOMINATED NODE
logging-es-data-master-55lp74ix-1-jms4g   2/2       Running   0          1m        10.129.0.24   {{ INFRA_INTERNAL_FQDN }}    <none>
logging-fluentd-2pc7j                     1/1       Running   0          1m        10.128.2.4    {{ NODE2_INTERNAL_FQDN }}   <none>
logging-fluentd-6pl9r                     1/1       Running   0          1m        10.131.2.4    {{ NODE5_INTERNAL_FQDN }}   <none>
logging-fluentd-7nd2l                     1/1       Running   0          1m        10.131.0.4    {{ NODE1_INTERNAL_FQDN }}   <none>
logging-fluentd-gvkbv                     1/1       Running   0          1m        10.130.0.6    {{ NODE3_INTERNAL_FQDN }}   <none>
logging-fluentd-ptqvs                     1/1       Running   0          1m        10.129.2.5    {{ NODE4_INTERNAL_FQDN }}   <none>
logging-fluentd-qb42p                     1/1       Running   0          1m        10.130.2.6    {{ NODE6_INTERNAL_FQDN }}   <none>
logging-fluentd-tdczj                     1/1       Running   0          1m        10.128.0.6    {{ MASTER_INTERNAL_FQDN }}   <none>
logging-fluentd-tn9ww                     1/1       Running   0          1m        10.129.0.22   {{ INFRA_INTERNAL_FQDN }}    <none>
logging-kibana-1-b54pv                    2/2       Running   0          2m        10.129.0.21   {{ INFRA_INTERNAL_FQDN }}    <none>
----

The _Fluentd_ *Pods* are deployed as part of a *DaemonSet*, which is a mechanism
to ensure that specific *Pods* run on specific *Nodes* in the cluster at all
times:

[source,bash,role="copypaste"]
----
oc get daemonset
----

You will see something like:

----
NAME              DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                AGE
logging-fluentd   8         8         8         8            8           logging-infra-fluentd=true   3m
----

You will also see the storage for ElasticSearch being automatically
provisioned from the OCS block storage service if you query the
PersistentVolumeClaim objects in this project

[source,bash,role="copypaste"]
----
oc get pvc
----

And you will see something like:

[source,bash,role="copypaste"]
----
NAME           STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS               AGE
logging-es-0   Bound     pvc-8188d8dd-6af7-11e8-af61-02cea7838d26   10Gi       RWO            {{ CNS_BLOCK_STORAGECLASS }}   3m
----

[NOTE]
====
Much like with the Metrics solution, we defined the appropriate
`NodeSelector` in the Logging configuration to ensure that the Logging
components only landed on the infra node. That being said, the `DaemonSet`
ensures FluentD runs on *all* nodes. Otherwise we would not capture all of
the container logs.
====

To reach the _Kibana_ user interface, first determine its public access URL by
querying the *Route* that got set up to expose Kibana's *Service*:

[source,bash,role="copypaste"]
----
oc get route/logging-kibana
----

You will see something like:

----
NAME             HOST/PORT                                              PATH      SERVICES         PORT      TERMINATION          WILDCARD
logging-kibana   kibana.apps.{{ OCP_ROUTING_SUFFIX }}             logging-kibana   <all>     reencrypt/Redirect   None
----

You can click the link ( https://kibana.{{ OCP_ROUTING_SUFFIX }} ) to open the
Kibana interface. There is a special authentication proxy that is configured as
part of the EFK installation that results in Kibana requiring OpenShift
credentials for access. You should login to Kibana as the `fancyuser1` user with password
`openshift` to be able to see all of the cluster's logs. Kibana utilizes the same RBAC
underpinning OpenShift to ensure that users can only see the logs they should
have access to.

[IMPORTANT]
====
The block-storage service of OCS (also referred to as `gluster-block`, introduced in the next chapter) is **only** supported for Logging and Metrics as of this release. This is about to change in the near future as we qualify more workloads.
====

### OpenShift Multitenant Networking
OpenShift has a software defined network (SDN) inside the platform that is based
on Open vSwitch. This SDN is used to provide connectivity between application
components inside of the OpenShift environment. It comes with default network
ranges pre-configured, although you can make changes to these should they
conflict with your existing infrastructure, or for whatever other reason you may
have.

When you installed OpenShift, there was an option set in the installer's
configuration to enable the multitenant network plugin:

[source,ini]
----
os_sdn_network_plugin_name=redhat/openshift-ovs-multitenant
----

The OpenShift Multitenant SDN plug-in enables a true isolated multi-tenant
network infrastructure inside OpenShift’s software defined network. While you
have seen projects isolate resources through OpenShift’s RBAC, the multitenant
SDN plugin isolates projects using separate virtual network IDs within Open
vSwitch.

The multitenant network plugin was introduced in OpenShift 3.1, and more
information about it and its configuration can be found in the
link:https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html[networking
documentation^]. Additionally, other vendors are working with the upstream
Kubernetes community to implement their own SDN plugins, and several of these
are supported by the vendors for use with OpenShift. These plugin
implementations make use of appc/CNI, which is outside the scope of this lab.

#### Execute the Creation Script
Only users with cluster administration privileges can manipulate *Project*
networks. First, make sure you are logged in as the cluster administrator:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

Then, execute a script that we have prepared for you. It will create two
*Projects* and then deploy a *DeploymentConfig* with a *Pod* for you:

[source,bash,role="copypaste"]
----
bash /opt/lab/support/net-proj.sh
----

#### Examine Network Namespaces
Two *Projects* were created for you, `netproj-a` and `netproj-b`. Execute the
following command to see the network namespaces:

[source,bash,role="copypaste"]
----
oc get netnamespaces
----

You will see something like the following:

[source]
----
NAME                    NETID      EGRESS IPS
app-management          10765501   []
default                 0          []
infra-storage           12129484   []
kube-public             3885278    []
kube-system             7128412    []
management-infra        15734027   []
netproj-a               11910878   []
netproj-b               6478895    []
...
----

Note that each project has its own network namespace with a unique ID. The
`default` project is a special exception. Its network ID is 0. This network is a
global network. It is joined (not isolated) to all other networks in the SDN by
default. If you remember from earlier exercises, the OpenShift router and the
image registry are both in the `default` project. This means that *Pods* in all
other projects can access them. That's good, because the router needs to be able
to proxy traffic to the *Pods* to make them accessible from outside of
OpenShift.

#### Test Connectivity
Now that you have some networks and pods, you will need to find the IP address
of the pod in the `netproj-b` *Project*. Make sure that the pod is `Running` in the `netproj-b` namespace:

[source,bash,role="copypaste"]
----
oc get pod -n netproj-b
----

Once it's running, the following command will show you the IP address:

[source,bash,role="copypaste"]
----
bash /opt/lab/support/podbip.sh
----

The output will simply be the IP address of the pod in the `netproj-b` project.
The everyday way to do this would be with a combination of the `get` and
`describe` verbs. Feel free to do the following to verify what the script did:

[source,bash,role="copypaste copypaste-warning"]
----
oc get pod -n netproj-b
oc describe pod ose-1-f0deb
----

Make sure to substitute the correct pod name in the describe command.

`describe` will show you a lot of information about the pod, including its IP
address on the software defined network. Either way, make note of the IP address
you found above. It will look something like _10.1.4.12_.

Export the IP address of your pod into a shell variable like so:

[source,bash,role="copypaste copypaste-warning"]
----
export POD_B_IP=10.1.4.12
----

Make sure to use the correct IP address that you saw earlier in the command
output.

The OpenShift command-line tool and the web console provide mechanisms to
execute commands inside *Pods* running in the environment. This is a useful
feature for both developers as well as for cluster and application
operators/administrators. You will use that feature in order to test network
connectivity between the two *Pods* you created.

Get the name of the *Pod* running in the `netproj-a` *Project*:

[source,bash,role="copypaste"]
----
oc get pods -n netproj-a
----

Then, export the *Pod* ID as a shell variable:

[source,bash,role="copypaste copypaste-warning"]
----
export POD_A_NAME=ose-1-q9mt5
----

Be sure to use the name that you saw in the output of your command.

Now, go ahead and `exec` a `ping` command inside *Pod* A, trying to reach *Pod*
B:

[source,bash,role="copypaste"]
----
oc exec -n netproj-a $POD_A_NAME -- ping -c1 -W1 $POD_B_IP
----

Your `ping` output should look like the following:

----
PING 10.129.0.10 (10.129.0.10) 56(84) bytes of data.

--- 10.129.0.10 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
----

You will see 100% packet loss (your `ping` command sends 1 packet, waits 1 second,
and gets no response). This is because the networks are not connected to one
another. Now simply execute the following:

[source,bash,role="copypaste"]
----
ping -c1 -W1 $POD_B_IP
----

You will see a successful ping. This is because the master (the system you are
on) is also a node attached to the SDN. At the host level you are able to reach
across all networks, virtual or otherwise. This is important to keep in mind
when you consider the overall network-level security of your cluster. Someone
logged in to an OpenShift host can "see" and touch everything on the SDN.

#### Join the Networks
Now it’s time to join the networks. Execute the following:

[source,bash,role="copypaste"]
----
oc get netnamespace
----

Take note of the network IDs for `netproj-a` and `netproj-b`. Then:

[source,bash,role="copypaste"]
----
oc adm pod-network join-projects netproj-a --to=netproj-b
----

And then look at the network IDs again:

[source,bash,role="copypaste"]
----
oc get netnamespace
----

You should see that the network IDs of the two projects are now the same.

#### Retest Connectivity
Now that the projects are joined, your `ping` between the pods should work.
Execute the original `ping` test again:

[source,bash,role="copypaste"]
----
oc exec -n netproj-a $POD_A_NAME -- ping -c1 -W1 $POD_B_IP
----

This time, your packet should reach its destination:

----
PING 10.129.0.10 (10.129.0.10) 56(84) bytes of data.
64 bytes from 10.129.0.10: icmp_seq=1 ttl=64 time=1.07 ms

--- 10.129.0.10 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.075/1.075/1.075/0.000 ms
----

#### Isolate Projects
Now, go ahead and isolate (unjoin) the projects, and then run your `ping` again:

[source,bash,role="copypaste"]
----
oc adm pod-network isolate-projects netproj-a
oc exec -n netproj-a $POD_A_NAME -- ping -c1 -W1 $POD_B_IP
----

You should see that your `ping` fails again.

Network multitenancy is a bit of a blunt tool. You can either give total access
between two projects, or completely restrict access. Don't fret, though. If you
need finer-grained control of inter-*Pod* and *Service* communication, there is
a Tech Preview network implementation called `NetworkPolicy`. You can learn more
about it in the
link:https://docs.openshift.com/container-platform/3.11/admin_guide/managing_networking.html#admin-guide-networking-networkpolicy[product
documentation].

### Node Maintenance

It is possible to put any node of the OpenShift environment into maintenance by
marking it as non-schedulable followed by a _drain_ of all pods on the node.

These operations require elevated privileges. Ensure you are logged in as
cluster admin:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

You will see by now that there are pods running on almost all of your nodes:

[source,bash,role="copypaste"]
----
oc get pods --all-namespaces -o wide
----

Sometimes you might need to perform maintenance on a host. Let's take a look
at the *Pods* that are on `node02`:

[source,bash,role="copypaste"]
----
oc adm manage-node --list-pods {{ NODE2_INTERNAL_FQDN }}
----

Firstly, we probably want to ensure that no new workload can be put on this
host. Mark node `{{ NODE2_INTERNAL_FQDN }}` as non-schedulable to prevent the
schedulers in the system to place any new workloads on it:

[source,bash,role="copypaste"]
----
oc adm manage-node {{ NODE2_INTERNAL_FQDN }} --schedulable=false
----

The output of the command will show that the node is now not schedulable:

----
NAME                                          STATUS                     ROLES     AGE       VERSION
{{ NODE2_INTERNAL_FQDN }}   Ready,SchedulingDisabled   compute   1h        v1.11.0+d4cacc0
----

Marking the node as non-schedulable did not impact the pods it is running. List those
pods:

[source,bash,role="copypaste"]
----
oc adm manage-node {{ NODE2_INTERNAL_FQDN }} --list-pods
----

Other than a *Pod* for Container Native Storage and a Fluentd instance (there is
one on every node), there may or may not be other *Pods* running on this node.

The next step is to drain the *Pods* to other nodes in the cluster.

[IMPORTANT]
====
*Pods* running on the node as part of a `DaemonSet` like those associated to
Logging or OCS will *not* be drained. They will not be accessible anymore
through OpenShift, but will continue to run as containers on the nodes until the
local OpenShift services are stopped and/or the node is shutdown. This is not a
problem since software like OCS or the OpenShift Metrics stack is designed to
handle such situations transparently.
====

Start the drain process like this:

[source,bash,role="copypaste"]
----
oc adm drain {{ NODE2_INTERNAL_FQDN }} --ignore-daemonsets
----

After a few moments, all of the *Pods*, except those for Fluentd, Container
Native Storage, and Prometheus previously running on `{{ NODE2_INTERNAL_FQDN
}}` should have terminated and been launched elsewhere.

[source,bash,role="copypaste"]
----
oc adm manage-node {{ NODE2_INTERNAL_FQDN }} --list-pods
----

The node `{{ NODE2_INTERNAL_FQDN }}` is now ready for an administrator to
start maintenance operations. If those include a reboot of the system or
upgrading OpenShift components, the *Pods* associated with
OCS and logging will come back up automatically.

Now that our maintenance is complete, the node is still non-schedulable. Let's
fix that:

[source,bash,role="copypaste"]
----
oc adm manage-node {{ NODE2_INTERNAL_FQDN }} --schedulable=true
----

Now the node will be able to have workload scheduled on it again:

----
NAME                                          STATUS    ROLES     AGE       VERSION
{{ NODE2_INTERNAL_FQDN }}   Ready     compute   1h        v1.11.0+d4cacc0
----

### Running the OpenShift Registry with OCS

The Registry in OpenShift is a critical component. As it is the default
destination for all container builds in the cluster, and is the source for
deploying applications built inside the cluster, being unavailable is a big
problem.

The internal registry runs as one or more *Pods* inside the OpenShift
environment. By default the registry uses local ephemeral storage in its *Pod*.
This means that any restarts or re-deployments or outages would cause all of the
built/pushed container images to be lost. Also, only having one registry
instance and/or one infrastructure node could cause temporary outages. So,
adding storage and scaling up the registry is a good idea.

[IMPORTANT]
====
Your cluster only has one infrastructure node. In practice, you would want a
minimum of three to achieve high-availability for all infrastructure services.
====

#### Adding OCS to the Registry
Adding storage to the registry is as easy as it was for our file-uploader
application. Simply make the registry *Pods* use a PVC in access mode *RWX*
based on OCS. This way, a highly-available scale-out registry can be provided
without external dependencies on NFS or Cloud Provider storage.

[IMPORTANT]
====
The following method will be disruptive. All data stored in the registry so far
will be lost (the Rails and PHP app images). Migration scenarios exist but are
beyond the scope of this lab, but normally you would configure persistent
storage for the registry before starting to really use your cluster.
====

Make sure you are logged in as `system:admin` in the `default` namespace:

[source,bash,role="copypaste"]
----
oc login -u system:admin -n default
----

Just like with the file uploader example, you can simply add a volume (and have
its *PersistentVolumeClaim* created automatically) with the `oc set volume` command.
Execute the following:

[source,bash,role="copypaste"]
----
oc set volume dc/docker-registry --add --name=registry-storage -t pvc \
--claim-mode=ReadWriteMany --claim-size=5Gi \
--claim-name=registry-storage --claim-class={{ CNS_INFRA_STORAGECLASS }} --overwrite
----

The registry will now redeploy.

[NOTE]
====
The registry is preconfigured with a volume called `registry-storage` that is
using the `emptyDir` storage type. The above command will `--overwrite` the existing
volume with our new PVC. More information can be found in the
link:https://docs.openshift.com/container-platform/3.11/dev_guide/volumes.html[volumes
documentation^].
====

[TIP]
====
It is also possible to use `openshift-ansible` to deploy the registry
====

After a couple of seconds a new deployment of the registry should be available.
Verify a new version of the registry's *DeploymentConfig* is running:

[source,bash,role="copypaste"]
----
oc get dc/docker-registry
----

Wait until you see the following state:

----
NAME              REVISION   DESIRED   CURRENT   TRIGGERED BY
docker-registry   2          1         1         config
----

Now your OpenShift Registry is using persistent storage provided by OCS.  Since
this is shared storage this also allows you to scale out the registry pods.

You can scale the registry like this:

[source,bash,role="copypaste"]
----
oc scale dc/docker-registry --replicas=3
----

After a short while you should see 3 healthy registry pods in the default
*Project*:

[source,bash,role="copypaste"]
----
oc get pods
----

And you should see something like:

----
NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-2-5rszg    1/1       Running   0          1m
docker-registry-2-7s3tm    1/1       Running   0          14s
docker-registry-2-g3l70    1/1       Running   0          14s
registry-console-1-b47jt   1/1       Running   0          6h
router-1-hs9wp             1/1       Running   0          6h
----

Check the registry's `DeploymentConfig` to verify it indeeds mounts a `PersistentVolume` to the `/registry` directory which is where the registry stores all container images:

[source,bash,role="copypaste"]
----
oc describe dc docker-registry
----

This should show:

----
Name:		docker-registry
Namespace:	default
Created:	2 hours ago
Labels:		docker-registry=default
Annotations:	<none>
Latest Version:	2
Selector:	docker-registry=default
Replicas:	3
Triggers:	Config
Strategy:	Rolling
Template:
Pod Template:
  Labels:		docker-registry=default
  Service Account:	registry
  Containers:
   registry:
    Image:	support.internal.aws.testdrive.openshift.com:5000/openshift3/ose-docker-registry:v3.11.16
    Port:	5000/TCP
    Requests:
      cpu:	100m
      memory:	256Mi
    Liveness:	http-get https://:5000/healthz delay=10s timeout=5s period=10s #success=1 #failure=3
    Readiness:	http-get https://:5000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    Environment:
      REGISTRY_HTTP_ADDR:					:5000
      REGISTRY_HTTP_NET:					tcp
      REGISTRY_HTTP_SECRET:					g4fMc23QUZLFhRmtu7m7mCah5bhefi3h2sBPbjgJvdw=
      REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA:	false
      REGISTRY_OPENSHIFT_SERVER_ADDR:				docker-registry.default.svc:5000
      REGISTRY_HTTP_TLS_KEY:					/etc/secrets/registry.key
      REGISTRY_HTTP_TLS_CERTIFICATE:				/etc/secrets/registry.crt
    Mounts:
      /etc/secrets from registry-certificates (rw)
      /registry from registry-storage (rw) <1>
  Volumes:
   registry-certificates:
    Type:	Secret (a volume populated by a Secret)
    SecretName:	registry-certificates
    Optional:	false
   registry-storage: <2>
    Type:	PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) <3>
    ClaimName:	registry-storage <4>
    ReadOnly:	false

Deployment #2 (latest):
	Name:		docker-registry-2
	Created:	48 seconds ago
	Status:		Complete
	Replicas:	3 current / 3 desired
	Selector:	deployment=docker-registry-2,deploymentconfig=docker-registry,docker-registry=default
	Labels:		docker-registry=default,openshift.io/deployment-config.name=docker-registry
	Pods Status:	3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Deployment #1:
	Created:	2 hours ago
	Status:		Complete
	Replicas:	0 current / 0 desired

Events:
  Type		Reason				Age	From				Message
  ----		------				----	----				-------
  Normal	DeploymentCreated		48s	deploymentconfig-controller	Created new replication controller "docker-registry-2" for version 2
  Normal	ReplicationControllerScaled	29s	deploymentconfig-controller	Scaled replication controller "docker-registry-2" from 1 to 3
----
<1> The `/registry` directory in the pod namespace will be a mountpoint for a `PersistentVolume` called `registry-storage`
<2> The definition for the volume `registry-storage`
<3> The volume will be of the type `PersistentVolume` which is referenced to a `PersistentVolumeClaim`
<4> The name of the `PersistentVolumeClaim` which this volume references
